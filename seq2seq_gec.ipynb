{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6uK1ehNF9pm",
    "outputId": "639f952d-61e3-469a-bed6-48538814d7cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in ./.venv/lib/python3.10/site-packages (4.7.1)\n",
      "Requirement already satisfied: six in ./.venv/lib/python3.10/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from gdown) (3.12.2)\n",
      "Requirement already satisfied: requests[socks] in ./.venv/lib/python3.10/site-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (from gdown) (4.65.0)\n",
      "Requirement already satisfied: beautifulsoup4 in ./.venv/lib/python3.10/site-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests[socks]->gdown) (2.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests[socks]->gdown) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.5.7)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in ./.venv/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown --no-cache-dir -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5d_kNrSGDZ8"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HPHyhqQ29SZH"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fZ2NTiDl-Kb4"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import pathlib as pl\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPvXtF9kAPeP"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vphVs_6J-j_6",
    "outputId": "a5614fc7-eee1-42a9-93c9-e9e93a72d465"
   },
   "outputs": [],
   "source": [
    "!gdown \"18d7-qbKjt2uS1ORdvVIr8LBrTqdZYaTI\"\n",
    "!tar xvjf \"/content/C4_200M.hdf5-00001.3-of-00010.tar.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kXVftQJS-Mot"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Hdf5Dataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading entries from HDF5 databases\"\"\"\n",
    "\n",
    "    def __init__(self, h5_path, transform=None, num_entries=None):\n",
    "        self.h5f = h5py.File(h5_path, \"r\")\n",
    "        if num_entries:\n",
    "            self.num_entries = num_entries\n",
    "        else:\n",
    "            self.num_entries = self.h5f[\"labels\"].shape[0]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index > self.num_entries:\n",
    "            raise StopIteration\n",
    "        input = self.h5f[\"input\"][index].decode(\"utf-8\")\n",
    "        label = self.h5f[\"labels\"][index].decode(\"utf-8\")\n",
    "        if self.transform is not None:\n",
    "            features = self.transform(input)\n",
    "        return input, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVNak5A4ATbV"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wnkb5Uhv-Pod"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seang/Dev/Git/NMA2023/Grammar-Error-Correction/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterable, List\n",
    "from tqdm import tqdm\n",
    "import pathlib as pl\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, index: int) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "    for data_sample in tqdm(data_iter):\n",
    "        if data_sample[index] and isinstance(data_sample[index], str):\n",
    "            yield token_transform(data_sample[index])\n",
    "\n",
    "SRC_LANGUAGE = \"incorrect\"\n",
    "TGT_LANGUAGE = \"correct\"\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "VOCAB_SIZE = 20000\n",
    "N_TRAIN_SAMPLES = 1000000\n",
    "N_VAL_SAMPLES = 100000\n",
    "\n",
    "# Place-holders\n",
    "token_transform = get_tokenizer(\"basic_english\")\n",
    "vocab_transform = None\n",
    "\n",
    "folder = \"./data\"\n",
    "train_filename = \"C4_200M.hdf5-00000-of-00010\"\n",
    "valid_filename = \"C4_200M.hdf5-00001-of-00010\"\n",
    "\n",
    "embedding_path = \"./glove.42B.300d.txt\"\n",
    "\n",
    "checkpoint_folder = \"./checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kX06GSI9GcKX",
    "outputId": "5047c692-dfef-4e3b-af1f-3964043d5f6e"
   },
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "gdown.download_folder(\n",
    "    \"https://drive.google.com/drive/folders/1FQ_jm765fgwcD5lLtjl6ef9k532hdADR\",\n",
    "    quiet=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JPeUeY0dA6YP"
   },
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = [\"<UNK>\", \"<PAD>\", \"<BOS>\", \"<EOS>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YDDjmMopOBqz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.42B.300d.zip: 1.88GB [05:53, 5.32MB/s]                                                                                                                                   \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 19999/20000 [00:01<00:00, 11888.61it/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import GloVe, vocab\n",
    "\n",
    "def pretrained_embs(name: str, dim: str, max_vectors: int = None):\n",
    "    glove_vectors = GloVe(name=name, dim=dim, max_vectors=max_vectors)\n",
    "    glove_vocab = vocab(glove_vectors.stoi)\n",
    "    pretrained_embeddings = glove_vectors.vectors\n",
    "    glove_vocab.insert_token(\"<UNK>\", UNK_IDX)\n",
    "    pretrained_embeddings = torch.cat(\n",
    "        (torch.mean(pretrained_embeddings, dim=0, keepdims=True), pretrained_embeddings)\n",
    "    )\n",
    "    glove_vocab.insert_token(\"<PAD>\", PAD_IDX)\n",
    "    pretrained_embeddings = torch.cat(\n",
    "        (torch.zeros(1, pretrained_embeddings.shape[1]), pretrained_embeddings)\n",
    "    )\n",
    "    glove_vocab.insert_token(\"<BOS>\", PAD_IDX)\n",
    "    pretrained_embeddings = torch.cat(\n",
    "        (torch.rand(1, pretrained_embeddings.shape[1]), pretrained_embeddings)\n",
    "    )\n",
    "    glove_vocab.insert_token(\"<EOS>\", PAD_IDX)\n",
    "    pretrained_embeddings = torch.cat(\n",
    "        (torch.rand(1, pretrained_embeddings.shape[1]), pretrained_embeddings)\n",
    "    )\n",
    "    glove_vocab.set_default_index(UNK_IDX)\n",
    "    return glove_vocab, pretrained_embeddings\n",
    "\n",
    "vocab, embeddings = pretrained_embs(\"42B\", \"300\", 20000)\n",
    "\n",
    "torch.save(embeddings, \"glove.42B.300d.20K.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2gr7PgrPOrGj"
   },
   "outputs": [],
   "source": [
    "# Load vocabulary and pretrained embeddings\n",
    "\n",
    "vocab_transform = torch.load(\"vocab/vocab_20K.pth\")\n",
    "embeddings = torch.load(\"glove.42B.300d.20K.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDx_3gmfxUuN"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4An4xCwA-8k"
   },
   "source": [
    "## Collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Gr8ARUKFA8zO"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat(\n",
    "        (torch.tensor([BOS_IDX]), torch.tensor(token_ids), torch.tensor([EOS_IDX]))\n",
    "    )\n",
    "\n",
    "# src and tgt language text transforms to convert raw strings into tensors indices\n",
    "text_transform = sequential_transforms(\n",
    "    token_transform, vocab_transform, tensor_transform\n",
    ")  # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform(src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform(tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VZPC2VdeBAL5",
    "outputId": "85b11680-5434-4c13-e03f-d61e184dcd3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized input:\n",
      " ['data', 'mining', 'is', 'awesome', '!']\n",
      "encoded input:\n",
      " [157, 1185, 13, 1480, 32]\n",
      "transformed input:\n",
      " tensor([   2,  157, 1185,   13, 1480,   32,    3])\n"
     ]
    }
   ],
   "source": [
    "text = \"data mining is awesome!\"\n",
    "tokenized_input = token_transform(text)\n",
    "print(\"tokenized input:\\n\", tokenized_input)\n",
    "\n",
    "encoded_input = vocab_transform(tokenized_input)\n",
    "print(\"encoded input:\\n\", encoded_input)\n",
    "\n",
    "print(\"transformed input:\\n\", text_transform(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbpCULVLBFby"
   },
   "source": [
    "## Unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iN82KeuGBHdb",
    "outputId": "6ae46bec-0cf5-49f6-8ac0-6b4dd6c77e23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataminingisawesome', '!']\n",
      "[0, 32]\n"
     ]
    }
   ],
   "source": [
    "text = \"dataminingisawesome!\"\n",
    "tokenized_input = token_transform(text)\n",
    "print(tokenized_input)\n",
    "\n",
    "encoded_input = vocab_transform(tokenized_input)\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jswthNvABLAT"
   },
   "source": [
    "RNN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "4rarHnwuBL0r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "teacher_forcing_ratio = 0.5\n",
    "torch.manual_seed(0)\n",
    "\n",
    "EMB_SIZE = 300\n",
    "HIDDEN_SIZE = 512\n",
    "BATCH_SIZE = 16\n",
    "NUM_ENCODER_LAYERS = 1\n",
    "NUM_DECODER_LAYERS = 1\n",
    "\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1RLZ_q2iBQIV"
   },
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = (\n",
    "        mask.float()\n",
    "        .masked_fill(mask == 0, float(\"-inf\"))\n",
    "        .masked_fill(mask == 1, float(0.0))\n",
    "    )\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src):\n",
    "    src_seq_len = src.shape[0]\n",
    "\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, src_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HZI1idP-BUe0"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for src, trg in tqdm(iterator):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        src = src.to(DEVICE)\n",
    "        trg = trg.to(DEVICE)\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for src, trg in tqdm(iterator):\n",
    "\n",
    "            src = src.to(DEVICE)\n",
    "            trg = trg.to(DEVICE)\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "QN6QA4x_W2j9"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from random import random\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout, embedding_weights=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=1)\n",
    "\n",
    "        if embedding_weights is not None:\n",
    "            self.embedding.weight = torch.nn.Parameter(\n",
    "                torch.from_numpy(embedding_weights)\n",
    "            )\n",
    "\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "\n",
    "        embedded = self.dropout(self.embedding(src).float())\n",
    "\n",
    "        # embedded = [src len, batch size, emb dim]\n",
    "\n",
    "        outputs, hidden = self.rnn(embedded)  # no cell state!\n",
    "\n",
    "        # outputs = [src len, batch size, hid dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        # outputs are always from the top hidden layer\n",
    "\n",
    "        return hidden\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, dropout, embedding_weights=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=1)\n",
    "\n",
    "        if embedding_weights is not None:\n",
    "            self.embedding.weight = torch.nn.Parameter(\n",
    "                torch.from_numpy(embedding_weights)\n",
    "            )\n",
    "\n",
    "        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\n",
    "\n",
    "        # self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden, context):\n",
    "        # input = [batch size]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        # context = [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        # n layers and n directions in the decoder will both always be 1, therefore:\n",
    "        # hidden = [1, batch size, hid dim]\n",
    "        # context = [1, batch size, hid dim]\n",
    "\n",
    "        input = input.unsqueeze(0)\n",
    "\n",
    "        # input = [1, batch size]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(input).float())\n",
    "\n",
    "        # embedded = [1, batch size, emb dim]\n",
    "        emb_con = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        # emb_con = [1, batch size, emb dim + hid dim]\n",
    "        output, hidden = self.rnn(emb_con, hidden)\n",
    "\n",
    "        # output = [seq len, batch size, hid dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        # seq len, n layers and n directions will always be 1 in the decoder, therefore:\n",
    "        # output = [1, batch size, hid dim]\n",
    "        # hidden = [1, batch size, hid dim]\n",
    "\n",
    "        output = torch.cat(\n",
    "            (embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), dim=1\n",
    "        )\n",
    "\n",
    "        # output = [batch size, emb dim + hid dim * 2]\n",
    "\n",
    "        prediction = self.softmax(self.fc_out(output))\n",
    "\n",
    "        # prediction = [batch size, output dim]\n",
    "\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "9fCOYTOKBW9v"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "        assert (\n",
    "            encoder.hid_dim == decoder.hid_dim\n",
    "        ), \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src = [src len, batch size]\n",
    "        # trg = [trg len, batch size]\n",
    "        # teacher_forcing_ratio is probability to use teacher forcing\n",
    "        # e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # last hidden state of the encoder is the context\n",
    "        context = self.encoder(src)\n",
    "\n",
    "        # context also used as the initial hidden state of the decoder\n",
    "        hidden = context\n",
    "\n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = trg[0, :]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            # insert input token embedding, previous hidden state and the context state\n",
    "            # receive output tensor (predictions) and new hidden state\n",
    "            output, hidden = self.decoder(input, hidden, context)\n",
    "\n",
    "            # place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "\n",
    "            # decide if we are going to use teacher forcing or not\n",
    "            teacher_force = np.random.rand() < teacher_forcing_ratio\n",
    "\n",
    "            # get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            # if teacher forcing, use actual next token as next input\n",
    "            # if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WfDp_4o7BXh2",
    "outputId": "34405a40-b14d-422b-9f36-40faae239540"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 41,789,440 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# attn = Attention(HIDDEN_SIZE, HIDDEN_SIZE)\n",
    "\n",
    "encoder1 = Encoder(\n",
    "    VOCAB_SIZE, EMB_SIZE, HIDDEN_SIZE, 0, embedding_weights=np.array(embeddings)\n",
    ")\n",
    "decoder1 = Decoder(\n",
    "    VOCAB_SIZE, EMB_SIZE, HIDDEN_SIZE, 0.1, embedding_weights=np.array(embeddings)\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "# optimizer = torch.optim.Adam(encoder1.parameters(), lr = learning_rate , betas=(0.9, 0.98), eps=1e-9)\n",
    "# decoder_optimizer = torch.optim.Adam(encoder1.parameters(), lr = learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "model = Seq2Seq(encoder1, decoder1, DEVICE)\n",
    "model = nn.DataParallel(model, device_ids=[0,1,2,3])\n",
    "model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-27-23:50:44\n"
     ]
    }
   ],
   "source": [
    "current_time = lambda: time.strftime(\"%Y-%m-%d-%H:%M:%S\", time.localtime())\n",
    "print(current_time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "id": "K9_i-sP-TPDO",
    "outputId": "1bfc18f8-e0c5-46b6-f056-0689c0eade11",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mEpoch 1 of 1 - time: 2023-07-27-23:50:47\u001b[0m\n",
      "\u001b[92mTraining...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████████████████▏                                                                                                                           | 9109/62500 [37:27<3:40:05,  4.04it/s]"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 1\n",
    "CLIP = 1 # gradient clipping\n",
    "RESUME = False\n",
    "\n",
    "train_iter = Hdf5Dataset(\n",
    "    pl.Path(folder) / train_filename, num_entries=N_TRAIN_SAMPLES)\n",
    "train_dataloader = DataLoader(\n",
    "    train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "val_iter = Hdf5Dataset(pl.Path(folder) / valid_filename, num_entries=N_VAL_SAMPLES)\n",
    "val_dataloader = DataLoader(\n",
    "    val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "# make sure folder exists\n",
    "pl.Path(\"checkpoints\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model.train()\n",
    "if RESUME:\n",
    "    checkpoint = torch.load(\n",
    "        pl.Path(\"checkpoints\") /\n",
    "        f\"model-epoch_{NUM_EPOCHS-1}-{current_time()}.pt\"\n",
    "    )\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    start_time = timer()\n",
    "    print(\n",
    "        f\"\\033[92mEpoch {epoch} of {NUM_EPOCHS} - time: {current_time()}\\033[0m\")\n",
    "    print(f\"\\033[92mTraining...\\033[0m\")\n",
    "    train_loss = train(model, train_dataloader, optimizer, loss_fn, 0)\n",
    "    end_time = timer()\n",
    "    print(f\"\\033[92mValidating...\\033[0m\")\n",
    "    val_loss = evaluate(model, val_dataloader, loss_fn)\n",
    "    print(\n",
    "        (\n",
    "            f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"\n",
    "            f\"Epoch time = {(end_time - start_time):.3f}s\"\n",
    "        )\n",
    "    )\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"loss\": val_loss,\n",
    "        },\n",
    "        pl.Path(\"checkpoints\") /\n",
    "        f\"model-epoch_{NUM_EPOCHS-1}-{current_time()}.pt\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0TE0FdC5VOBx"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# function to generate output sequence using greedy algorithm\n",
    "def correct_sentence_vectorized(src_tensor, model, max_len=50):\n",
    "    assert isinstance(src_tensor, torch.Tensor)\n",
    "\n",
    "    model.eval()\n",
    "    src_tensor = src_tensor.unsqueeze(1).to(DEVICE)\n",
    "    # get length of input sentence\n",
    "    src_len = src_tensor.shape[0]\n",
    "\n",
    "    trg_vocab_size = model.decoder.output_dim\n",
    "\n",
    "    # tensor to store decoder outputs\n",
    "    outputs = torch.zeros(max_len, 1, trg_vocab_size).to(DEVICE)\n",
    "\n",
    "    # last hidden state of the encoder is the context\n",
    "    with torch.no_grad():\n",
    "        context = model.encoder(src_tensor)\n",
    "\n",
    "    # context also used as the initial hidden state of the decoder\n",
    "    hidden = context\n",
    "\n",
    "    # first input to the decoder is the <sos> tokens\n",
    "    input = src_tensor[0, :]\n",
    "    # enc_src = [batch_sz, src_len, hid_dim]\n",
    "    # Even though some examples might have been completed by producing a <eos> token\n",
    "    # we still need to feed them through the model because other are not yet finished\n",
    "    # and all examples act as a batch. Once every single sentence prediction encounters\n",
    "    # <eos> token, then we can stop predicting.\n",
    "    for t in range(1, max_len):\n",
    "        # insert input token embedding, previous hidden state and the context state\n",
    "        # receive output tensor (predictions) and new hidden state\n",
    "        output, hidden = model.decoder(input, hidden, context)\n",
    "\n",
    "        # place predictions in a tensor holding predictions for each token\n",
    "        outputs[t] = output\n",
    "\n",
    "        # get the highest predicted token from our predictions\n",
    "        top1 = output.argmax(1)\n",
    "\n",
    "        # if teacher forcing, use actual next token as next input\n",
    "        # if not, use predicted token\n",
    "        input = top1\n",
    "\n",
    "    pred_sentence = []\n",
    "\n",
    "    for i in range(1, len(outputs)):\n",
    "        topv, topi = outputs[i, :, :].topk(1)\n",
    "        pred_sentence.append(vocab_transform.vocab.itos_[topi])\n",
    "        if topi == EOS_IDX:\n",
    "            break\n",
    "\n",
    "    return \" \".join(pred_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3dDyDJ4UVeL4",
    "outputId": "dd58acf2-4979-4799-8072-fe06c6981d4a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "latest_checkpoint = sorted(Path(\"checkpoints\").glob(\"*.pt\"), key=os.path.getmtime)[-1]\n",
    "\n",
    "checkpoint = torch.load(latest_checkpoint)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Pick one in 18M examples\n",
    "val_iter = Hdf5Dataset(pl.Path(folder) / valid_filename, num_entries=None)\n",
    "\n",
    "src, trg = random.choice(val_iter)\n",
    "\n",
    "print('input: \"', src, '\"')\n",
    "print('target: \"', trg, '\"')\n",
    "\n",
    "src = text_transform(src)\n",
    "\n",
    "print(f\"\\033[91mModel's prediction: \\033[0m\", end=\"\")\n",
    "print(correct_sentence_vectorized(src, model))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
