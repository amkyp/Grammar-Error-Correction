{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6uK1ehNF9pm",
    "outputId": "639f952d-61e3-469a-bed6-48538814d7cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in ./.venv/lib/python3.10/site-packages (4.7.1)\n",
      "Requirement already satisfied: six in ./.venv/lib/python3.10/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from gdown) (3.12.2)\n",
      "Requirement already satisfied: requests[socks] in ./.venv/lib/python3.10/site-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (from gdown) (4.65.0)\n",
      "Requirement already satisfied: beautifulsoup4 in ./.venv/lib/python3.10/site-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests[socks]->gdown) (2.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests[socks]->gdown) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.5.7)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in ./.venv/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown --no-cache-dir -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5d_kNrSGDZ8"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "fZ2NTiDl-Kb4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "from typing import Iterable, List\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import pathlib as pl\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data import get_tokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.vocab import GloVe, vocab\n",
    "from torchinfo import summary\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the data\n",
    "\n",
    "Get the data from google drive and extract it into the correct directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vphVs_6J-j_6",
    "outputId": "a5614fc7-eee1-42a9-93c9-e9e93a72d465"
   },
   "outputs": [],
   "source": [
    "!gdown \"18d7-qbKjt2uS1ORdvVIr8LBrTqdZYaTI\"\n",
    "!tar xvjf \"./data/C4_200M.hdf5-00001.3-of-00010.tar.bz2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Object\n",
    "\n",
    "Create an HDF5 dataset object which will be fed into the DataLoader later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kXVftQJS-Mot"
   },
   "outputs": [],
   "source": [
    "class Hdf5Dataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading entries from HDF5 databases\"\"\"\n",
    "\n",
    "    def __init__(self, h5_path: str, transform = None, num_entries: int = None):\n",
    "        \"\"\"Initialization function. Obtains the raw data as bytes, num entries, and transform, if any\n",
    "        \n",
    "        :param h5_path: a string representing a valid path to the hdf5 file to read\n",
    "        :param transform: a function that is used to process the raw input (optional)\n",
    "        :param num_entries: number of entries in the dataset (optional)\n",
    "        :returns: an object of the Hdf5Datset\n",
    "        \"\"\"\n",
    "        self.h5f = h5py.File(h5_path, \"r\") # open HDF5 file for reading\n",
    "        if num_entries:\n",
    "            self.num_entries = num_entries\n",
    "        else: # if num_entries not given, set len of dataset to the len of the lables in the hdf5 file\n",
    "            self.num_entries = self.h5f[\"labels\"].shape[0]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"obtains both the input as well as the labels at the given index. index may\n",
    "           be an integer or slice\n",
    "        \n",
    "        :param index: integer or slice to index dataset with\n",
    "        :returns: a tuple containing either a single input,label pair or list of inputs, list of labes pair\n",
    "        \"\"\"\n",
    "        if isinstance(index, slice):\n",
    "            input = [entry.decode(\"utf-8\") for entry in self.h5f[\"input\"][index]]\n",
    "            label = [entry.decode(\"utf-8\") for entry in self.h5f[\"labels\"][index]]\n",
    "        elif isinstance(index, int):\n",
    "            if index > self.num_entries:\n",
    "                raise StopIteration\n",
    "            input = self.h5f[\"input\"][index].decode(\"utf-8\")\n",
    "            label = self.h5f[\"labels\"][index].decode(\"utf-8\")\n",
    "        # if self.transform is not None:\n",
    "        #     features = self.transform(input)\n",
    "        return input, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"gets number of entries\"\"\"\n",
    "        return self.num_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing with hdf5 data importing in python\n",
    "root_path = \"./\"\n",
    "data_path = f\"{root_path}data/\"\n",
    "c4_hdf5_train_filename = \"C4_200M.hdf5-00000-of-00010\"\n",
    "c4_hdf5_train_filepath = f\"{data_path}{c4_hdf5_train_filename}\"\n",
    "c4_dataset = Hdf5Dataset(c4_hdf5_train_filepath) # no transforms yet, no len given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Preheat oven to 356 degrees Fahrenheit. (180 & degrees celsius), I don’t use a macro.wave so I put the coconut oil in the oven to melt it while preheated. (lifehack :-)) Some extra coconut oil to green coconut oil to grease your pan,',\n",
       " 'Preheat oven to 356 degrees Fahrenheit (180 degrees celsius). I don’t have a microwave so I put the coconut oil in the oven to melt it while it is preheating (lifehack :-)). Use some extra coconut oil to grease your pan.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c4_dataset)\n",
    "c4_dataset[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables\n",
    "\n",
    "includes input parameters, vocabulary parameters, filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wnkb5Uhv-Pod"
   },
   "outputs": [],
   "source": [
    "# def yield_tokens(data_iter: Iterable, index: int, src_lang_lbl: str, tgt_lang_lbl: str) -> List[str]:\n",
    "#     \"\"\"Helper function to yield a list of tokens from a data iterable\"\"\"\n",
    "#     language_index = {src_lang_lbl: 0, tgt_lang_lbl: 1}\n",
    "#     for data_sample in tqdm(data_iter):\n",
    "#         if data_sample[index] and isinstance(data_sample[index], str):\n",
    "#             yield token_transform(data_sample[index])\n",
    "\n",
    "SRC_LANGUAGE = \"incorrect\"\n",
    "TGT_LANGUAGE = \"correct\"\n",
    "\n",
    "MAX_LENGTH = 512 # maximum length of an input sequence\n",
    "VOCAB_SIZE = 20000 # size of the vocabulary, something to play around with\n",
    "N_TRAIN_SAMPLES = 1000000 # number of training samples to pull from the dataset\n",
    "N_VAL_SAMPLES = int(0.1 * N_TRAIN_SAMPLES) # number of validation/test samples to pull from the dataset\n",
    "\n",
    "# function-name placeholders for transforms\n",
    "token_transform = get_tokenizer(\"basic_english\")\n",
    "vocab_transform = None\n",
    "\n",
    "folder = \"./data\"\n",
    "train_filename = \"C4_200M.hdf5-00000-of-00010\"\n",
    "valid_filename = \"C4_200M.hdf5-00001-of-00010\"\n",
    "\n",
    "embedding_path = \"./glove.42B.300d.txt\"\n",
    "\n",
    "checkpoint_folder = \"./checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kX06GSI9GcKX",
    "outputId": "5047c692-dfef-4e3b-af1f-3964043d5f6e"
   },
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "gdown.download_folder(\n",
    "    \"https://drive.google.com/drive/folders/1FQ_jm765fgwcD5lLtjl6ef9k532hdADR\",\n",
    "    quiet=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary\n",
    "\n",
    "Create special tokens then create pre-trained GloVe vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JPeUeY0dA6YP"
   },
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = [\"<UNK>\", \"<PAD>\", \"<BOS>\", \"<EOS>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YDDjmMopOBqz"
   },
   "outputs": [],
   "source": [
    "def pretrained_embs(name: str, dim: str, max_vectors: int = None):\n",
    "    \"\"\"get the pre-trained word embeddings for the vocabulary\n",
    "    \n",
    "    :param name: the name of the embedding, according to torchtext.vocab.GloVe\n",
    "    :param dim: the desired dimensionality of the embedding\n",
    "    :param max_vectors: maximum number of pre-trained vectors to load\n",
    "    :returns: a tuple containing the str-to-int vocab dict and the embedded vectors of the vocab\n",
    "    \"\"\"\n",
    "    glove_vectors = GloVe(name=name, dim=dim, max_vectors=max_vectors)\n",
    "    glove_vocab = vocab(glove_vectors.stoi)\n",
    "    pretrained_embeddings = glove_vectors.vectors\n",
    "    glove_vocab.insert_token(\"<UNK>\", UNK_IDX)\n",
    "    pretrained_embeddings = torch.cat(\n",
    "        (torch.mean(pretrained_embeddings, dim=0, keepdims=True), pretrained_embeddings)\n",
    "    )\n",
    "    glove_vocab.insert_token(\"<PAD>\", PAD_IDX)\n",
    "    pretrained_embeddings = torch.cat(\n",
    "        (torch.zeros(1, pretrained_embeddings.shape[1]), pretrained_embeddings)\n",
    "    )\n",
    "    glove_vocab.insert_token(\"<BOS>\", PAD_IDX)\n",
    "    pretrained_embeddings = torch.cat(\n",
    "        (torch.rand(1, pretrained_embeddings.shape[1]), pretrained_embeddings)\n",
    "    )\n",
    "    glove_vocab.insert_token(\"<EOS>\", PAD_IDX)\n",
    "    pretrained_embeddings = torch.cat(\n",
    "        (torch.rand(1, pretrained_embeddings.shape[1]), pretrained_embeddings)\n",
    "    )\n",
    "    glove_vocab.set_default_index(UNK_IDX)\n",
    "    return glove_vocab, pretrained_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.42B.300d.zip: 1.88GB [05:53, 5.32MB/s]                                                                                                                                                                                                                    \n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 19999/20000 [00:01<00:00, 14166.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.vocab.vocab.Vocab'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "vocab, embeddings = pretrained_embs(\"42B\", \"300\", 20000)\n",
    "print(type(vocab), type(embeddings))\n",
    "torch.save(embeddings, \"glove.42B.300d.20K.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2gr7PgrPOrGj"
   },
   "outputs": [],
   "source": [
    "# Load vocabulary and pretrained embeddings\n",
    "vocab_transform = torch.load(\"vocab/vocab_20K.pth\")\n",
    "embeddings = torch.load(\"glove.42B.300d.20K.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDx_3gmfxUuN"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4An4xCwA-8k"
   },
   "source": [
    "## Collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Gr8ARUKFA8zO"
   },
   "outputs": [],
   "source": [
    "def sequential_transforms(*transforms):\n",
    "    \"\"\"Helper function to compose sequential operations\n",
    "    \n",
    "    param transforms: list of transformation functions to be applied\n",
    "    :returns: the full transformation after composition of sequential ops\n",
    "    \"\"\"\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    \"\"\"prepends and appends the given list of token ids with BOS id and EOS\n",
    "    \n",
    "    param token_ids: a list of integers representing the ids of the tokens in a sequence\n",
    "    :returns: a pytorch tensor representation of the concatenated BOS id, ids list, and EOS id\n",
    "    \"\"\"\n",
    "    return torch.cat(\n",
    "        (torch.tensor([BOS_IDX]), torch.tensor(token_ids), torch.tensor([EOS_IDX]))\n",
    "    )\n",
    "\n",
    "# src and tgt language text transforms to convert raw strings into tensors indices\n",
    "# with BOS and EOS prepended and appended resp.\n",
    "text_transform = sequential_transforms(\n",
    "    token_transform, vocab_transform, tensor_transform\n",
    ")\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform(src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform(tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VZPC2VdeBAL5",
    "outputId": "85b11680-5434-4c13-e03f-d61e184dcd3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized input:\n",
      " ['according', 'to', 'all', 'known', 'laws', 'of', 'aviation', ',', 'there', 'is', 'no', 'way', 'a', 'bee', 'should', 'be', 'able', 'to', 'fly', '.']\n",
      "encoded input:\n",
      " [306, 8, 42, 529, 1888, 9, 4717, 5, 70, 13, 81, 138, 10, 6663, 129, 28, 315, 8, 2702, 4]\n",
      "transformed input:\n",
      " tensor([   2,  306,    8,   42,  529, 1888,    9, 4717,    5,   70,   13,   81,\n",
      "         138,   10, 6663,  129,   28,  315,    8, 2702,    4,    3])\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"According to all known laws of aviation, there\n",
    "          is no way a bee should be able to fly.\"\"\"\n",
    "tokenized_input = token_transform(text)\n",
    "print(\"tokenized input:\\n\", tokenized_input)\n",
    "\n",
    "encoded_input = vocab_transform(tokenized_input)\n",
    "print(\"encoded input:\\n\", encoded_input)\n",
    "\n",
    "print(\"transformed input:\\n\", text_transform(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbpCULVLBFby"
   },
   "source": [
    "## Unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iN82KeuGBHdb",
    "outputId": "6ae46bec-0cf5-49f6-8ac0-6b4dd6c77e23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lmk', 'where', 'ur', 'at']\n",
      "[0, 116, 11025, 23]\n",
      "transformed input:\n",
      " tensor([    2,     0,   116, 11025,    23,     3])\n"
     ]
    }
   ],
   "source": [
    "text = \"lmk where ur at\"\n",
    "tokenized_input = token_transform(text)\n",
    "print(tokenized_input)\n",
    "\n",
    "encoded_input = vocab_transform(tokenized_input)\n",
    "print(encoded_input)\n",
    "\n",
    "print(\"transformed input:\\n\", text_transform(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jswthNvABLAT"
   },
   "source": [
    "## RNN Network\n",
    "\n",
    "### Masking\n",
    "\n",
    "We will definitely need a padding mask, and if we use self-attention (maybe later) we will also need a subsequent mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1RLZ_q2iBQIV"
   },
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz, device='cpu'):\n",
    "    \"\"\"Create a mask that ignores future inputs\n",
    "\n",
    "    :param sz: the size the input sequence to which the mask will be applied\n",
    "    :returns: a tensor with shape (sz, sz)\n",
    "    \"\"\"\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "    mask = (\n",
    "        mask.float()\n",
    "        .masked_fill(mask == 0, float(\"-inf\"))\n",
    "        .masked_fill(mask == 1, float(0.0))\n",
    "    )\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_padding_mask(src):\n",
    "    \"\"\"creates a padding mask which ignores PAD tokens when applied to an input\n",
    "\n",
    "    :param src: the input sequence to compute the mask for\n",
    "    :returns: a tensor with shape (src_seq_len, src_seq_len)\n",
    "    \"\"\"\n",
    "    src_seq_len = src.shape[0]\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    return src_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_size = 10\n",
    "subseq_mask = generate_square_subsequent_mask(example_size)\n",
    "subseq_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example text ids before padding:\n",
      " tensor([    2,     0,   116, 11025,    23,     3])\n",
      "\n",
      "example text ids after padding:\n",
      " tensor([[    2,     0,   116, 11025,    23,     3,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "\n",
      "example text padding mask:\n",
      " tensor([[False, False, False, False, False, False,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "example_text = \"lmk where ur at\"\n",
    "example_text_id_seq = text_transform(example_text.rstrip(\"\\n\"))\n",
    "\n",
    "example_max_len = 50\n",
    "num_pad = example_max_len - len(example_text_id_seq)\n",
    "padded_example_text_id_seq = torch.cat((example_text_id_seq, torch.tensor([PAD_IDX] * num_pad)))\n",
    "padded_example_text_id_seq = padded_example_text_id_seq.view(-1,1) # turn the tensor into a column vec\n",
    "example_padding_mask = create_padding_mask(padded_example_text_id_seq)\n",
    "print(\"example text ids before padding:\\n\", example_text_id_seq)\n",
    "print()\n",
    "print(\"example text ids after padding:\\n\", padded_example_text_id_seq.T)\n",
    "print()\n",
    "print(\"example text padding mask:\\n\", example_padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-Specific Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "teacher_forcing_ratio = 0.5\n",
    "torch.manual_seed(0)\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "EMB_SIZE = 300\n",
    "HIDDEN_SIZE = 512\n",
    "BATCH_SIZE = 16\n",
    "NUM_ENCODER_LAYERS = 1\n",
    "NUM_DECODER_LAYERS = 1\n",
    "\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "The RNN we employ contains encoder block(s) and a decoder block(s) which themselves are comprised of GRU units.\\\n",
    "We Apply an embedding layer before the encoder/decoder blocks, and then we apply a single dropout layer for the\\\n",
    "encoding layer, and a fully connected output layer, followed by drop-out and softmax in the decoding layer.\n",
    "\n",
    "#### TODO:\n",
    "\n",
    "- check that the embeddings are not trainable\n",
    "- double-check the architecture of the encoder\n",
    "- double check the architecture of the decoder, particularly the fc outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "QN6QA4x_W2j9"
   },
   "outputs": [],
   "source": [
    "def init_seq2seq(module):\n",
    "    \"\"\"Initialize weights for seq2seq\"\"\"\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "    if type(module) == nn.GRU:\n",
    "        for param in module._flat_weights_names:\n",
    "            if \"weight\" in param:\n",
    "                nn.init.xavier_uniform_(module._parameters[param])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_sz, emb_dim, hid_dim, num_layers, dropout=0, embedding_weights=None):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.embedding = nn.Embedding(vocab_sz, emb_dim, padding_idx=1)\n",
    "        if embedding_weights is not None:\n",
    "            self.embedding.weight = torch.nn.Parameter(\n",
    "                torch.from_numpy(embedding_weights)\n",
    "            )\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, num_layers=num_layers, batch_first=False, dropout=dropout)\n",
    "        self.apply(init_seq2seq)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src.t().type(torch.int64))\n",
    "        outputs, hidden_state = self.rnn(embedded)  # no cell state!\n",
    "        # outputs = [src len, batch size, hid dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        # outputs are always from the top hidden layer\n",
    "        return outputs, hidden_state\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_sz, emb_dim, hid_dim, num_layers, dropout=0, embedding_weights=None):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.output_dim = vocab_sz\n",
    "        self.embedding = nn.Embedding(vocab_sz, emb_dim, padding_idx=1)\n",
    "        if embedding_weights is not None:\n",
    "            self.embedding.weight = torch.nn.Parameter(\n",
    "                torch.from_numpy(embedding_weights)\n",
    "            )\n",
    "        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim, num_layers=num_layers, dropout=dropout)\n",
    "        self.fc_out = nn.LazyLinear(vocab_size)\n",
    "        #self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def init_state(self, enc_all_outputs, *args):\n",
    "        return enc_all_outputs\n",
    "    \n",
    "    def forward(self, input, state):\n",
    "        # input shape: (batch_size, num_t_steps)\n",
    "        # embs shape: (num_t_steps, batch_size, embed_size)\n",
    "        embs = self.embedding(input.t().type(torch.int64))\n",
    "        enc_output, hidden_state = state\n",
    "        # context shape: (batch_size, num_hiddens) (we grab the last output)\n",
    "        context = enc_output[-1]\n",
    "        # broadcast context to (num_t_steps, batch_size, num_hiddens)\n",
    "        context = context.repeat(embs.shape[0], 1, 1)\n",
    "        # concat along feature dimension\n",
    "        print(embs.shape)\n",
    "        print(context.shape)\n",
    "        embs_and_context = torch.cat((embs, context), -1)\n",
    "        outputs, hidden_state = self.rnn(embs_and_context, hidden_state)\n",
    "        outputs = self.fc_out(outputs).swapaxes(0, 1) #-> why swap?\n",
    "        return outputs, [enc_output, hidden_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing out the encoder and decoder\n",
    "vocab_size, embed_size, num_hiddens, num_layers = 10, 8, 16, 2\n",
    "batch_size, num_t_steps = 4, 9\n",
    "encoder = Encoder(vocab_size, embed_size, num_hiddens, num_layers)\n",
    "X = torch.zeros((batch_size, num_steps))\n",
    "enc_outputs, enc_state = encoder(X)\n",
    "d2l.check_shape(enc_outputs, (num_t_steps, batch_size, num_hiddens))\n",
    "d2l.check_shape(enc_state, (num_layers, batch_size, num_hiddens))\n",
    "\n",
    "decoder = Decoder(vocab_size, embed_size, num_hiddens, num_layers)\n",
    "dec_outputs, state = decoder(X, (enc_outputs, enc_state))\n",
    "d2l.check_shape(dec_outputs, (batch_size, num_t_steps, vocab_size))\n",
    "d2l.check_shape(state[1], (num_layers, batch_size, num_hiddens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "9fCOYTOKBW9v"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        assert (\n",
    "            encoder.hid_dim == decoder.hid_dim\n",
    "        ), \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing=0.5):\n",
    "        # enc_all_outputs = self.encoder(src, *args)\n",
    "        # dec_state = self.decoder.init_state(enc_all_outputs, *args)\n",
    "        # return self.decoder(trg, dec_state)[0]\n",
    "\n",
    "        # teacher_forcing_ratio is probability to use teacher forcing\n",
    "        # e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        # last hidden state of the encoder is the context\n",
    "        enc_outputs, state = self.encoder(src)\n",
    "        # first input to the decoder is the <bos> tokens\n",
    "        input = trg[0, :]\n",
    "        for t in range(1, trg_len):\n",
    "            # insert input token embedding, previous hidden state and the context state\n",
    "            # receive output tensor (predictions) and new hidden state\n",
    "            dec_outputs, state = self.decoder(input, (enc_outputs, state))\n",
    "            # place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = dec_outputs\n",
    "            # if teacher forcing, use actual next token as next input\n",
    "            # if not, use highest predicted token from predictions\n",
    "            input = trg[t] if np.random.rand() < teacher_forcing else dec_outputs.argmax(-1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WfDp_4o7BXh2",
    "outputId": "34405a40-b14d-422b-9f36-40faae239540"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 300])\n",
      "torch.Size([512, 16, 512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Encoder: 1, Embedding: 2, GRU: 2, Embedding: 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/Dev/Git/Grammar-Error-Correction/.venv/lib/python3.10/site-packages/torchinfo/torchinfo.py:295\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 295\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/Dev/Git/Grammar-Error-Correction/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1538\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "Cell \u001b[0;32mIn[57], line 30\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[0;34m(self, src, trg, teacher_forcing)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, trg_len):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# insert input token embedding, previous hidden state and the context state\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# receive output tensor (predictions) and new hidden state\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     dec_outputs, state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# place predictions in a tensor holding predictions for each token\u001b[39;00m\n",
      "File \u001b[0;32m~/Dev/Git/Grammar-Error-Correction/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1538\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "Cell \u001b[0;32mIn[80], line 59\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, input, state)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(context\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 59\u001b[0m embs_and_context \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43membs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m outputs, hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(embs_and_context, hidden_state)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 3",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     18\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, betas\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.98\u001b[39m), eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-9\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_LENGTH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_LENGTH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#print(f\"The model has {count_parameters(model):,} trainable parameters\")\u001b[39;00m\n",
      "File \u001b[0;32m~/Dev/Git/Grammar-Error-Correction/.venv/lib/python3.10/site-packages/torchinfo/torchinfo.py:223\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m validate_user_params(\n\u001b[1;32m    217\u001b[0m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[1;32m    218\u001b[0m )\n\u001b[1;32m    220\u001b[0m x, correct_input_size \u001b[38;5;241m=\u001b[39m process_input(\n\u001b[1;32m    221\u001b[0m     input_data, input_size, batch_dim, device, dtypes\n\u001b[1;32m    222\u001b[0m )\n\u001b[0;32m--> 223\u001b[0m summary_list \u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_forward_pass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m formatting \u001b[38;5;241m=\u001b[39m FormattingOptions(depth, verbose, columns, col_width, rows)\n\u001b[1;32m    227\u001b[0m results \u001b[38;5;241m=\u001b[39m ModelStatistics(\n\u001b[1;32m    228\u001b[0m     summary_list, correct_input_size, get_total_memory_used(x), formatting\n\u001b[1;32m    229\u001b[0m )\n",
      "File \u001b[0;32m~/Dev/Git/Grammar-Error-Correction/.venv/lib/python3.10/site-packages/torchinfo/torchinfo.py:304\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    303\u001b[0m     executed_layers \u001b[38;5;241m=\u001b[39m [layer \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m summary_list \u001b[38;5;28;01mif\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mexecuted]\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to run torchinfo. See above stack traces for more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuted layers up to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecuted_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Encoder: 1, Embedding: 2, GRU: 2, Embedding: 2]"
     ]
    }
   ],
   "source": [
    "# attn = Attention(HIDDEN_SIZE, HIDDEN_SIZE)\n",
    "\n",
    "encoder1 = Encoder(\n",
    "    VOCAB_SIZE, EMB_SIZE, HIDDEN_SIZE, NUM_ENCODER_LAYERS, dropout=0, embedding_weights=np.array(embeddings)\n",
    ")\n",
    "decoder1 = Decoder(\n",
    "    VOCAB_SIZE, EMB_SIZE, HIDDEN_SIZE, NUM_DECODER_LAYERS, dropout=0.1, embedding_weights=np.array(embeddings)\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "# optimizer = torch.optim.Adam(encoder1.parameters(), lr = learning_rate , betas=(0.9, 0.98), eps=1e-9)\n",
    "# decoder_optimizer = torch.optim.Adam(encoder1.parameters(), lr = learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "model = Seq2Seq(encoder1, decoder1, DEVICE)\n",
    "#model = nn.DataParallel(model, device_ids=[0,1,2,3])\n",
    "model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "summary(model, input_size=[(BATCH_SIZE, MAX_LENGTH), (BATCH_SIZE, MAX_LENGTH)], device='cuda')\n",
    "#print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, loss_fn, clip):\n",
    "    model.train() # set model into training mode so apply dropout, etc.\n",
    "    epoch_loss = 0\n",
    "    for src, trg in tqdm(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        src = src.to(DEVICE)\n",
    "        trg = trg.to(DEVICE)\n",
    "        output = model(src, trg)\n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        loss = loss_fn(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval() # set model into inference mode, so DONT apply dropout\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, trg in tqdm(iterator):\n",
    "            src = src.to(DEVICE)\n",
    "            trg = trg.to(DEVICE)\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size]\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-28-07:01:45\n"
     ]
    }
   ],
   "source": [
    "current_time = lambda: time.strftime(\"%Y-%m-%d-%H:%M:%S\", time.localtime())\n",
    "print(current_time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "id": "K9_i-sP-TPDO",
    "outputId": "1bfc18f8-e0c5-46b6-f056-0689c0eade11",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mEpoch 1 of 1 - time: 2023-07-27-23:50:47\u001b[0m\n",
      "\u001b[92mTraining...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|█████████████████████████████████████████████████████████████████████████████████████████▋                                                    | 39489/62500 [2:42:08<1:24:09,  4.56it/s]"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 1\n",
    "CLIP = 1 # gradient clipping\n",
    "RESUME = False\n",
    "\n",
    "train_iter = Hdf5Dataset(\n",
    "    pl.Path(folder) / train_filename, num_entries=N_TRAIN_SAMPLES)\n",
    "train_dataloader = DataLoader(\n",
    "    train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "val_iter = Hdf5Dataset(pl.Path(folder) / valid_filename, num_entries=N_VAL_SAMPLES)\n",
    "val_dataloader = DataLoader(\n",
    "    val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "# make sure folder exists\n",
    "pl.Path(\"checkpoints\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model.train()\n",
    "if RESUME:\n",
    "    checkpoint = torch.load(\n",
    "        pl.Path(\"checkpoints\") /\n",
    "        f\"model-epoch_{NUM_EPOCHS-1}-{current_time()}.pt\"\n",
    "    )\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    start_time = timer()\n",
    "    print(\n",
    "        f\"\\033[92mEpoch {epoch} of {NUM_EPOCHS} - time: {current_time()}\\033[0m\")\n",
    "    print(f\"\\033[92mTraining...\\033[0m\")\n",
    "    train_loss = train(model, train_dataloader, optimizer, loss_fn, CLIP)\n",
    "    end_time = timer()\n",
    "    print(f\"\\033[92mValidating...\\033[0m\")\n",
    "    val_loss = evaluate(model, val_dataloader, loss_fn)\n",
    "    print(\n",
    "        (\n",
    "            f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"\n",
    "            f\"Epoch time = {(end_time - start_time):.3f}s\"\n",
    "        )\n",
    "    )\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"loss\": val_loss,\n",
    "        },\n",
    "        pl.Path(\"checkpoints\") /\n",
    "        f\"model-epoch_{NUM_EPOCHS-1}-{current_time()}.pt\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "0TE0FdC5VOBx"
   },
   "outputs": [],
   "source": [
    "# function to generate output sequence using greedy algorithm\n",
    "def correct_sentence_vectorized(src_tensor, model, max_len=50):\n",
    "    assert isinstance(src_tensor, torch.Tensor)\n",
    "\n",
    "    model.eval()\n",
    "    src_tensor = src_tensor.unsqueeze(1).to(DEVICE)\n",
    "    # get length of input sentence\n",
    "    src_len = src_tensor.shape[0]\n",
    "\n",
    "    trg_vocab_size = model.decoder.output_dim\n",
    "\n",
    "    # tensor to store decoder outputs\n",
    "    outputs = torch.zeros(max_len, 1, trg_vocab_size).to(DEVICE)\n",
    "\n",
    "    # last hidden state of the encoder is the context\n",
    "    with torch.no_grad():\n",
    "        context = model.encoder(src_tensor)\n",
    "\n",
    "    # context also used as the initial hidden state of the decoder\n",
    "    hidden = context\n",
    "\n",
    "    # first input to the decoder is the <sos> tokens\n",
    "    input = src_tensor[0, :]\n",
    "    # enc_src = [batch_sz, src_len, hid_dim]\n",
    "    # Even though some examples might have been completed by producing a <eos> token\n",
    "    # we still need to feed them through the model because other are not yet finished\n",
    "    # and all examples act as a batch. Once every single sentence prediction encounters\n",
    "    # <eos> token, then we can stop predicting.\n",
    "    for t in range(1, max_len):\n",
    "        # insert input token embedding, previous hidden state and the context state\n",
    "        # receive output tensor (predictions) and new hidden state\n",
    "        output, hidden = model.decoder(input, hidden, context)\n",
    "\n",
    "        # place predictions in a tensor holding predictions for each token\n",
    "        outputs[t] = output\n",
    "\n",
    "        # get the highest predicted token from our predictions\n",
    "        top1 = output.argmax(1)\n",
    "\n",
    "        # if teacher forcing, use actual next token as next input\n",
    "        # if not, use predicted token\n",
    "        input = top1\n",
    "\n",
    "    pred_sentence = []\n",
    "\n",
    "    for i in range(1, len(outputs)):\n",
    "        topv, topi = outputs[i, :, :].topk(1)\n",
    "        pred_sentence.append(vocab_transform.vocab.itos_[topi])\n",
    "        if topi == EOS_IDX:\n",
    "            break\n",
    "\n",
    "    return \" \".join(pred_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3dDyDJ4UVeL4",
    "outputId": "dd58acf2-4979-4799-8072-fe06c6981d4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/model-epoch_38-2023-07-27-23:29:53.pt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Pick one in 18M examples\u001b[39;00m\n\u001b[1;32m     12\u001b[0m val_iter \u001b[38;5;241m=\u001b[39m Hdf5Dataset(pl\u001b[38;5;241m.\u001b[39mPath(folder) \u001b[38;5;241m/\u001b[39m valid_filename, num_entries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 14\u001b[0m src, trg \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m, src, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m, trg, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32mmtrand.pyx:936\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m, in \u001b[0;36mHdf5Dataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh5f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m][index]\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mh5f\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[index]\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28minput\u001b[39m)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Dev/Git/NMA2023/Grammar-Error-Correction/.venv/lib/python3.10/site-packages/h5py/_hl/group.py:366\u001b[0m, in \u001b[0;36mGroup.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Group(oid)\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m otype \u001b[38;5;241m==\u001b[39m h5i\u001b[38;5;241m.\u001b[39mDATASET:\n\u001b[0;32m--> 366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mDataset(oid, readonly\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m otype \u001b[38;5;241m==\u001b[39m h5i\u001b[38;5;241m.\u001b[39mDATATYPE:\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m datatype\u001b[38;5;241m.\u001b[39mDatatype(oid)\n",
      "File \u001b[0;32m~/Dev/Git/NMA2023/Grammar-Error-Correction/.venv/lib/python3.10/site-packages/h5py/_hl/base.py:285\u001b[0m, in \u001b[0;36mHLObject.file\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfile\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    284\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Return a File instance associated with this object \"\"\"\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m phil:\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m files\u001b[38;5;241m.\u001b[39mFile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1053\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "latest_checkpoint = sorted(Path(\"checkpoints\").glob(\"*.pt\"), key=os.path.getmtime)[-1]\n",
    "print(latest_checkpoint)\n",
    "checkpoint = torch.load(latest_checkpoint)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Pick one in 18M examples\n",
    "val_iter = Hdf5Dataset(pl.Path(folder) / valid_filename, num_entries=None)\n",
    "\n",
    "src, trg = np.random.choice(val_iter)\n",
    "\n",
    "print('input: \"', src, '\"')\n",
    "print('target: \"', trg, '\"')\n",
    "\n",
    "src = text_transform(src)\n",
    "\n",
    "print(f\"\\033[91mModel's prediction: \\033[0m\", end=\"\")\n",
    "print(correct_sentence_vectorized(src, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
