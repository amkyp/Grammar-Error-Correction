{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6uK1ehNF9pm",
    "outputId": "639f952d-61e3-469a-bed6-48538814d7cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in ./.venv/lib/python3.10/site-packages (4.7.1)\n",
      "Requirement already satisfied: six in ./.venv/lib/python3.10/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from gdown) (3.12.2)\n",
      "Requirement already satisfied: requests[socks] in ./.venv/lib/python3.10/site-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (from gdown) (4.65.0)\n",
      "Requirement already satisfied: beautifulsoup4 in ./.venv/lib/python3.10/site-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests[socks]->gdown) (2.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests[socks]->gdown) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.5.7)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in ./.venv/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown --no-cache-dir -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5d_kNrSGDZ8"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fZ2NTiDl-Kb4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "from typing import Iterable, List\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import pathlib as pl\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data import get_tokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.vocab import GloVe, vocab\n",
    "from torchinfo import summary\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the data\n",
    "\n",
    "Get the data from google drive and extract it into the correct directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vphVs_6J-j_6",
    "outputId": "a5614fc7-eee1-42a9-93c9-e9e93a72d465"
   },
   "outputs": [],
   "source": [
    "!gdown \"18d7-qbKjt2uS1ORdvVIr8LBrTqdZYaTI\"\n",
    "!tar xvjf \"./data/C4_200M.hdf5-00001.3-of-00010.tar.bz2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Object\n",
    "\n",
    "Create an HDF5 dataset object which will be fed into the DataLoader later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kXVftQJS-Mot"
   },
   "outputs": [],
   "source": [
    "class Hdf5Dataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading entries from HDF5 databases\"\"\"\n",
    "\n",
    "    def __init__(self, h5_path: str, transform = None, num_entries: int = None):\n",
    "        \"\"\"Initialization function. Obtains the raw data as bytes, num entries, and transform, if any\n",
    "        \n",
    "        :param h5_path: a string representing a valid path to the hdf5 file to read\n",
    "        :param transform: a function that is used to process the raw input (optional)\n",
    "        :param num_entries: number of entries in the dataset (optional)\n",
    "        :returns: an object of the Hdf5Datset\n",
    "        \"\"\"\n",
    "        self.h5f = h5py.File(h5_path, \"r\") # open HDF5 file for reading\n",
    "        if num_entries:\n",
    "            self.num_entries = num_entries\n",
    "        else: # if num_entries not given, set len of dataset to the len of the lables in the hdf5 file\n",
    "            self.num_entries = self.h5f[\"labels\"].shape[0]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"obtains both the input as well as the labels at the given index. index may\n",
    "           be an integer or slice\n",
    "        \n",
    "        :param index: integer or slice to index dataset with\n",
    "        :returns: a tuple containing either a single input,label pair or list of inputs, list of labes pair\n",
    "        \"\"\"\n",
    "        if isinstance(index, slice):\n",
    "            input = [entry.decode(\"utf-8\") for entry in self.h5f[\"input\"][index]]\n",
    "            label = [entry.decode(\"utf-8\") for entry in self.h5f[\"labels\"][index]]\n",
    "        elif isinstance(index, int):\n",
    "            if index > self.num_entries:\n",
    "                raise StopIteration\n",
    "            input = self.h5f[\"input\"][index].decode(\"utf-8\")\n",
    "            label = self.h5f[\"labels\"][index].decode(\"utf-8\")\n",
    "        # if self.transform is not None:\n",
    "        #     features = self.transform(input)\n",
    "        return input, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"gets number of entries\"\"\"\n",
    "        return self.num_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing with hdf5 data importing in python\n",
    "root_path = \"./\"\n",
    "data_path = f\"{root_path}data/\"\n",
    "c4_hdf5_train_filename = \"C4_200M.hdf5-00000-of-00010\"\n",
    "c4_hdf5_train_filepath = f\"{data_path}{c4_hdf5_train_filename}\"\n",
    "c4_dataset = Hdf5Dataset(c4_hdf5_train_filepath) # no transforms yet, no len given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Preheat oven to 356 degrees Fahrenheit. (180 & degrees celsius), I don’t use a macro.wave so I put the coconut oil in the oven to melt it while preheated. (lifehack :-)) Some extra coconut oil to green coconut oil to grease your pan,',\n",
       " 'Preheat oven to 356 degrees Fahrenheit (180 degrees celsius). I don’t have a microwave so I put the coconut oil in the oven to melt it while it is preheating (lifehack :-)). Use some extra coconut oil to grease your pan.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c4_dataset)\n",
    "c4_dataset[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables\n",
    "\n",
    "includes input parameters, vocabulary parameters, filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wnkb5Uhv-Pod"
   },
   "outputs": [],
   "source": [
    "# def yield_tokens(data_iter: Iterable, index: int, src_lang_lbl: str, tgt_lang_lbl: str) -> List[str]:\n",
    "#     \"\"\"Helper function to yield a list of tokens from a data iterable\"\"\"\n",
    "#     language_index = {src_lang_lbl: 0, tgt_lang_lbl: 1}\n",
    "#     for data_sample in tqdm(data_iter):\n",
    "#         if data_sample[index] and isinstance(data_sample[index], str):\n",
    "#             yield token_transform(data_sample[index])\n",
    "\n",
    "SRC_LANGUAGE = \"incorrect\"\n",
    "TGT_LANGUAGE = \"correct\"\n",
    "\n",
    "MAX_LENGTH = 512 # maximum length of an input sequence\n",
    "VOCAB_SIZE = 20000 # size of the vocabulary, something to play around with\n",
    "# debug nums\n",
    "N_TRAIN_SAMPLES = 10000 # number of training samples to pull from the dataset\n",
    "N_VAL_SAMPLES = int(0.1 * N_TRAIN_SAMPLES) # number of validation/test samples to pull from the dataset\n",
    "\n",
    "# function-name placeholders for transforms\n",
    "token_transform = get_tokenizer(\"basic_english\")\n",
    "vocab_transform = None\n",
    "\n",
    "folder = \"./data\"\n",
    "train_filename = \"C4_200M.hdf5-00000-of-00010\"\n",
    "valid_filename = \"C4_200M.hdf5-00001-of-00010\"\n",
    "\n",
    "embedding_path = \"./glove.42B.300d.txt\"\n",
    "\n",
    "checkpoint_folder = \"./checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kX06GSI9GcKX",
    "outputId": "5047c692-dfef-4e3b-af1f-3964043d5f6e"
   },
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "gdown.download_folder(\n",
    "    \"https://drive.google.com/drive/folders/1FQ_jm765fgwcD5lLtjl6ef9k532hdADR\",\n",
    "    quiet=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary\n",
    "\n",
    "Create special tokens then create pre-trained GloVe vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JPeUeY0dA6YP"
   },
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = [\"<UNK>\", \"<PAD>\", \"<BOS>\", \"<EOS>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YDDjmMopOBqz"
   },
   "outputs": [],
   "source": [
    "def pretrained_embs(name: str, dim: str, max_vectors: int = None):\n",
    "    \"\"\"get the pre-trained word embeddings for the vocabulary\n",
    "    \n",
    "    :param name: the name of the embedding, according to torchtext.vocab.GloVe\n",
    "    :param dim: the desired dimensionality of the embedding\n",
    "    :param max_vectors: maximum number of pre-trained vectors to load\n",
    "    :returns: a tuple containing the str-to-int vocab dict and the embedded vectors of the vocab\n",
    "    \"\"\"\n",
    "    glove_vectors = GloVe(name=name, dim=dim, max_vectors=max_vectors)\n",
    "    glove_vocab = vocab(glove_vectors.stoi)\n",
    "    pretrained_embeddings = glove_vectors.vectors\n",
    "    glove_vocab.insert_token(\"<UNK>\", UNK_IDX)\n",
    "    pretrained_embeddings = torch.cat(\n",
    "        (torch.mean(pretrained_embeddings, dim=0, keepdims=True), pretrained_embeddings)\n",
    "    )\n",
    "    glove_vocab.insert_token(\"<PAD>\", PAD_IDX)\n",
    "    pretrained_embeddings = torch.cat(\n",
    "        (torch.zeros(1, pretrained_embeddings.shape[1]), pretrained_embeddings)\n",
    "    )\n",
    "    glove_vocab.insert_token(\"<BOS>\", PAD_IDX)\n",
    "    pretrained_embeddings = torch.cat(\n",
    "        (torch.rand(1, pretrained_embeddings.shape[1]), pretrained_embeddings)\n",
    "    )\n",
    "    glove_vocab.insert_token(\"<EOS>\", PAD_IDX)\n",
    "    pretrained_embeddings = torch.cat(\n",
    "        (torch.rand(1, pretrained_embeddings.shape[1]), pretrained_embeddings)\n",
    "    )\n",
    "    glove_vocab.set_default_index(UNK_IDX)\n",
    "    return glove_vocab, pretrained_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.42B.300d.zip: 1.88GB [05:53, 5.32MB/s]                                                                                                                                                                                                                    \n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 19999/20000 [00:01<00:00, 14166.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.vocab.vocab.Vocab'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "vocab, embeddings = pretrained_embs(\"42B\", \"300\", 20000)\n",
    "print(type(vocab), type(embeddings))\n",
    "torch.save(embeddings, \"glove.42B.300d.20K.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2gr7PgrPOrGj"
   },
   "outputs": [],
   "source": [
    "# Load vocabulary and pretrained embeddings\n",
    "vocab_transform = torch.load(\"vocab/vocab_20K.pth\")\n",
    "embeddings = torch.load(\"glove.42B.300d.20K.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDx_3gmfxUuN"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4An4xCwA-8k"
   },
   "source": [
    "## Collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Gr8ARUKFA8zO"
   },
   "outputs": [],
   "source": [
    "def sequential_transforms(*transforms):\n",
    "    \"\"\"Helper function to compose sequential operations\n",
    "    \n",
    "    param transforms: list of transformation functions to be applied\n",
    "    :returns: the full transformation after composition of sequential ops\n",
    "    \"\"\"\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    \"\"\"prepends and appends the given list of token ids with BOS id and EOS\n",
    "    \n",
    "    param token_ids: a list of integers representing the ids of the tokens in a sequence\n",
    "    :returns: a pytorch tensor representation of the concatenated BOS id, ids list, and EOS id\n",
    "    \"\"\"\n",
    "    return torch.cat(\n",
    "        (torch.tensor([BOS_IDX]), torch.tensor(token_ids), torch.tensor([EOS_IDX]))\n",
    "    )\n",
    "\n",
    "# src and tgt language text transforms to convert raw strings into tensors indices\n",
    "# with BOS and EOS prepended and appended resp.\n",
    "text_transform = sequential_transforms(\n",
    "    token_transform, vocab_transform, tensor_transform\n",
    ")\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform(src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform(tgt_sample.rstrip(\"\\n\")))\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    # src_batch = torch.permute(src_batch, (1, 0))\n",
    "    # tgt_batch = torch.permute(tgt_batch, (1, 0))\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VZPC2VdeBAL5",
    "outputId": "85b11680-5434-4c13-e03f-d61e184dcd3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized input:\n",
      " ['according', 'to', 'all', 'known', 'laws', 'of', 'aviation', ',', 'there', 'is', 'no', 'way', 'a', 'bee', 'should', 'be', 'able', 'to', 'fly', '.']\n",
      "encoded input:\n",
      " [306, 8, 42, 529, 1888, 9, 4717, 5, 70, 13, 81, 138, 10, 6663, 129, 28, 315, 8, 2702, 4]\n",
      "transformed input:\n",
      " tensor([   2,  306,    8,   42,  529, 1888,    9, 4717,    5,   70,   13,   81,\n",
      "         138,   10, 6663,  129,   28,  315,    8, 2702,    4,    3])\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"According to all known laws of aviation, there\n",
    "          is no way a bee should be able to fly.\"\"\"\n",
    "tokenized_input = token_transform(text)\n",
    "print(\"tokenized input:\\n\", tokenized_input)\n",
    "\n",
    "encoded_input = vocab_transform(tokenized_input)\n",
    "print(\"encoded input:\\n\", encoded_input)\n",
    "\n",
    "print(\"transformed input:\\n\", text_transform(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbpCULVLBFby"
   },
   "source": [
    "## Unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iN82KeuGBHdb",
    "outputId": "6ae46bec-0cf5-49f6-8ac0-6b4dd6c77e23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lmk', 'where', 'ur', 'at']\n",
      "[0, 116, 11025, 23]\n",
      "transformed input:\n",
      " tensor([    2,     0,   116, 11025,    23,     3])\n"
     ]
    }
   ],
   "source": [
    "text = \"lmk where ur at\"\n",
    "tokenized_input = token_transform(text)\n",
    "print(tokenized_input)\n",
    "\n",
    "encoded_input = vocab_transform(tokenized_input)\n",
    "print(encoded_input)\n",
    "\n",
    "print(\"transformed input:\\n\", text_transform(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jswthNvABLAT"
   },
   "source": [
    "## RNN Network\n",
    "\n",
    "### Masking\n",
    "\n",
    "We will definitely need a padding mask, and if we use self-attention (maybe later) we will also need a subsequent mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1RLZ_q2iBQIV"
   },
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz, device='cpu'):\n",
    "    \"\"\"Create a mask that ignores future inputs\n",
    "\n",
    "    :param sz: the size the input sequence to which the mask will be applied\n",
    "    :returns: a tensor with shape (sz, sz)\n",
    "    \"\"\"\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "    mask = (\n",
    "        mask.float()\n",
    "        .masked_fill(mask == 0, float(\"-inf\"))\n",
    "        .masked_fill(mask == 1, float(0.0))\n",
    "    )\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_padding_mask(src):\n",
    "    \"\"\"creates a padding mask which ignores PAD tokens when applied to an input\n",
    "\n",
    "    :param src: the input sequence to compute the mask for\n",
    "    :returns: a tensor with shape (src_seq_len, src_seq_len)\n",
    "    \"\"\"\n",
    "    src_seq_len = src.shape[0]\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    return src_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_size = 10\n",
    "subseq_mask = generate_square_subsequent_mask(example_size)\n",
    "subseq_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example text ids before padding:\n",
      " tensor([    2,     0,   116, 11025,    23,     3])\n",
      "\n",
      "example text ids after padding:\n",
      " tensor([[    2,     0,   116, 11025,    23,     3,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "\n",
      "example text padding mask:\n",
      " tensor([[False, False, False, False, False, False,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "example_text = \"lmk where ur at\"\n",
    "example_text_id_seq = text_transform(example_text.rstrip(\"\\n\"))\n",
    "\n",
    "example_max_len = 50\n",
    "num_pad = example_max_len - len(example_text_id_seq)\n",
    "padded_example_text_id_seq = torch.cat((example_text_id_seq, torch.tensor([PAD_IDX] * num_pad)))\n",
    "padded_example_text_id_seq = padded_example_text_id_seq.view(-1,1) # turn the tensor into a column vec\n",
    "example_padding_mask = create_padding_mask(padded_example_text_id_seq)\n",
    "print(\"example text ids before padding:\\n\", example_text_id_seq)\n",
    "print()\n",
    "print(\"example text ids after padding:\\n\", padded_example_text_id_seq.T)\n",
    "print()\n",
    "print(\"example text padding mask:\\n\", example_padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-Specific Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "teacher_forcing_ratio = 0.5\n",
    "torch.manual_seed(0)\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "EMB_SIZE = 300\n",
    "HIDDEN_SIZE = 512\n",
    "BATCH_SIZE = 16\n",
    "NUM_ENCODER_LAYERS = 1\n",
    "NUM_DECODER_LAYERS = 1\n",
    "\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "The RNN we employ contains encoder block(s) and a decoder block(s) which themselves are comprised of GRU units.\\\n",
    "We Apply an embedding layer before the encoder/decoder blocks, and then we apply a single dropout layer for the\\\n",
    "encoding layer, and a fully connected output layer, followed by drop-out and softmax in the decoding layer.\n",
    "\n",
    "#### TODO:\n",
    "\n",
    "- check that the embeddings are not trainable\n",
    "- double-check the architecture of the encoder\n",
    "- double check the architecture of the decoder, particularly the fc outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QN6QA4x_W2j9"
   },
   "outputs": [],
   "source": [
    "def init_seq2seq(module):\n",
    "    \"\"\"Initialize weights for seq2seq\"\"\"\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "    if type(module) == nn.GRU:\n",
    "        for param in module._flat_weights_names:\n",
    "            if \"weight\" in param:\n",
    "                nn.init.xavier_uniform_(module._parameters[param])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout=0, embedding_weights=None):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.emb_dim = emb_dim\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=1)\n",
    "        if embedding_weights is not None:\n",
    "            self.embedding.weight = torch.nn.Parameter(\n",
    "                torch.from_numpy(embedding_weights)\n",
    "            )\n",
    "        self.gru = nn.GRU(emb_dim, hid_dim, num_layers=n_layers, batch_first=False, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.apply(init_seq2seq)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src shape: (seq_len, batch_size)\n",
    "        embedded = self.dropout(self.embedding(src.type(torch.int64)))\n",
    "        # embedded shape: (seq_len, batch_size, emb_dim)\n",
    "        outputs, state = self.gru(embedded)\n",
    "        # outputs shape: (seq_len, batch_size, hid_dim * n_dirs)\n",
    "        # state shape: (n_layers * n_dirs, batch_size, hid_dim)\n",
    "        return outputs, state\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout=0, embedding_weights=None,\n",
    "                 teacher_forcing=0.5):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.teacher_forcing = teacher_forcing\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=1)\n",
    "        if embedding_weights is not None:\n",
    "            self.embedding.weight = torch.nn.Parameter(\n",
    "                torch.from_numpy(embedding_weights)\n",
    "            )\n",
    "        # only have one layer, so dropout not actually added\n",
    "        self.gru = nn.GRU(emb_dim + hid_dim, hid_dim, num_layers=n_layers, batch_first=False, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.apply(init_seq2seq)\n",
    "        \n",
    "    def forward(self, input, context, hidden):\n",
    "        # input shape: (batch_size)\n",
    "        # hidden shape: (n_layers * n_dirs, batch_size, hid_dim)\n",
    "        input = input.unsqueeze(0)\n",
    "        # input shape: (1, batch_size)\n",
    "        embedded = self.dropout(self.embedding(input.type(torch.int64)))\n",
    "        # embedded shape: (1, batch_size, emb_dim)\n",
    "        emb_and_con = torch.cat((embedded, context), dim=-1)\n",
    "        output, hidden = self.gru(emb_and_con, hidden)\n",
    "        # output shape: (seq_len, batch_size, hid_dim * n_dirs)\n",
    "        # hidden shape: (n_layers * n_dirs, batch_size, hid_dim)\n",
    "\n",
    "        # will always have seq_len == n_dr == 1 in decoder (unless use self-attention\n",
    "        prediction = self.softmax(self.fc_out(output.squeeze(0)))\n",
    "        # prediction shape: (batch_size, output_dim)\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing out the encoder and decoder\n",
    "vocab_size, embed_size, num_hiddens, num_layers = 10, 8, 16, 1\n",
    "batch_size, num_t_steps = 4, 9\n",
    "encoder = Encoder(vocab_size, embed_size, num_hiddens, num_layers)\n",
    "example_src = torch.zeros(num_t_steps, batch_size)\n",
    "example_trg = torch.zeros(batch_size)\n",
    "enc_outputs, enc_hidden = encoder(example_src)\n",
    "d2l.check_shape(enc_outputs, (num_t_steps, batch_size, num_hiddens))\n",
    "d2l.check_shape(enc_hidden, (num_layers, batch_size, num_hiddens))\n",
    "\n",
    "decoder = Decoder(vocab_size, embed_size, num_hiddens, num_layers)\n",
    "dec_output, dec_hidden = decoder(example_trg, enc_hidden, enc_hidden)\n",
    "d2l.check_shape(dec_output, (batch_size, vocab_size)) # output for a single t step!\n",
    "d2l.check_shape(dec_hidden, (num_layers, batch_size, num_hiddens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "9fCOYTOKBW9v"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        assert (\n",
    "            encoder.hid_dim == decoder.hid_dim\n",
    "        ), \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert (\n",
    "            encoder.n_layers == decoder.n_layers\n",
    "        ), \"Number of layers of encoder and decoder must be equal!\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing=0.5):\n",
    "        # src shape: (num_t_steps, batch_size)\n",
    "        # trg shape: (num_t_steps, batch_size)\n",
    "        # teacher_forcing_ratio is probability to use teacher forcing\n",
    "        # e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        # last hidden state of the encoder is the context\n",
    "        trg_len, batch_size = trg.shape\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        # tensor to store outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        # obtain the hidden state from the encoder...\n",
    "        _, enc_hidden = self.encoder(src)\n",
    "        # ...which also servers as the input context and hidden state to the decoder\n",
    "        context = enc_hidden\n",
    "        dec_hidden = enc_hidden\n",
    "        # get the first batch-worth's tokens from the target (ie batch_size number of <BOS> tokens)\n",
    "        dec_input = trg[0, :]\n",
    "        for t in np.arange(1, trg_len+1):\n",
    "            dec_output, dec_hidden = self.decoder(dec_input, context, dec_hidden)\n",
    "            # dec_output is softmax regression over vocab ids, so grab id that maximizes prob\n",
    "            outputs[t-1] = dec_output # give outputs the probs: this is what softmax is trained on!\n",
    "            top_token_id = dec_output.argmax(1) # <- does this give (batch_size, vocab_sz) tensor??\n",
    "            if t < trg_len:\n",
    "                dec_input = trg[t] if np.random.rand() < teacher_forcing else top_token_id\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WfDp_4o7BXh2",
    "outputId": "34405a40-b14d-422b-9f36-40faae239540"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 25,549,440 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# attn = Attention(HIDDEN_SIZE, HIDDEN_SIZE)\n",
    "\n",
    "encoder1 = Encoder(\n",
    "    VOCAB_SIZE, EMB_SIZE, HIDDEN_SIZE, NUM_ENCODER_LAYERS, dropout=0, embedding_weights=np.array(embeddings)\n",
    ")\n",
    "decoder1 = Decoder(\n",
    "    VOCAB_SIZE, EMB_SIZE, HIDDEN_SIZE, NUM_DECODER_LAYERS, dropout=0.1, embedding_weights=np.array(embeddings)\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "model = Seq2Seq(encoder1, decoder1, DEVICE)\n",
    "model = nn.DataParallel(model, device_ids=[0,1])\n",
    "model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "#summary(model, input_size=[(MAX_LENGTH, 1), (MAX_LENGTH, 1)], device='cuda')\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, loss_fn, clip):\n",
    "    model.train() # set model into training mode so apply dropout, etc.\n",
    "    epoch_loss = 0\n",
    "    for src, trg in tqdm(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        src = src.to(DEVICE)\n",
    "        trg = trg.to(DEVICE)\n",
    "        output = model(src, trg)\n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        loss = loss_fn(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval() # set model into inference mode, so DONT apply dropout\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, trg in tqdm(iterator):\n",
    "            src = src.to(DEVICE)\n",
    "            trg = trg.to(DEVICE)\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size]\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-11-13:19:50\n"
     ]
    }
   ],
   "source": [
    "current_time = lambda: time.strftime(\"%Y-%m-%d-%H:%M:%S\", time.localtime())\n",
    "print(current_time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "id": "K9_i-sP-TPDO",
    "outputId": "1bfc18f8-e0c5-46b6-f056-0689c0eade11",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mEpoch 1 of 10 - time: 2023-08-11-13:19:51\u001b[0m\n",
      "\u001b[92mTraining...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [01:17<00:00,  8.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mValidating...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:04<00:00, 14.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 6.223, Val loss: 6.675, Epoch time = 77.602s\n",
      "\u001b[92mEpoch 2 of 10 - time: 2023-08-11-13:21:13\u001b[0m\n",
      "\u001b[92mTraining...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [01:15<00:00,  8.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mValidating...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:04<00:00, 14.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 4.630, Val loss: 6.658, Epoch time = 75.260s\n",
      "\u001b[92mEpoch 3 of 10 - time: 2023-08-11-13:22:33\u001b[0m\n",
      "\u001b[92mTraining...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [01:15<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mValidating...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:04<00:00, 14.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train loss: 3.856, Val loss: 6.568, Epoch time = 75.237s\n",
      "\u001b[92mEpoch 4 of 10 - time: 2023-08-11-13:23:52\u001b[0m\n",
      "\u001b[92mTraining...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [01:15<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mValidating...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:04<00:00, 14.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train loss: 3.468, Val loss: 6.565, Epoch time = 75.180s\n",
      "\u001b[92mEpoch 5 of 10 - time: 2023-08-11-13:25:12\u001b[0m\n",
      "\u001b[92mTraining...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [01:15<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mValidating...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:04<00:00, 14.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train loss: 3.200, Val loss: 6.542, Epoch time = 75.016s\n",
      "\u001b[92mEpoch 6 of 10 - time: 2023-08-11-13:26:32\u001b[0m\n",
      "\u001b[92mTraining...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [01:15<00:00,  8.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mValidating...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:04<00:00, 14.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train loss: 2.989, Val loss: 6.487, Epoch time = 75.083s\n",
      "\u001b[92mEpoch 7 of 10 - time: 2023-08-11-13:27:51\u001b[0m\n",
      "\u001b[92mTraining...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [01:15<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mValidating...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:04<00:00, 14.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train loss: 2.871, Val loss: 6.480, Epoch time = 75.058s\n",
      "\u001b[92mEpoch 8 of 10 - time: 2023-08-11-13:29:11\u001b[0m\n",
      "\u001b[92mTraining...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [01:15<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mValidating...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:04<00:00, 14.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train loss: 2.702, Val loss: 6.482, Epoch time = 75.077s\n",
      "\u001b[92mEpoch 9 of 10 - time: 2023-08-11-13:30:31\u001b[0m\n",
      "\u001b[92mTraining...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [01:14<00:00,  8.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mValidating...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:04<00:00, 14.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train loss: 2.617, Val loss: 6.443, Epoch time = 74.849s\n",
      "\u001b[92mEpoch 10 of 10 - time: 2023-08-11-13:31:50\u001b[0m\n",
      "\u001b[92mTraining...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [01:14<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mValidating...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:04<00:00, 14.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train loss: 2.511, Val loss: 6.428, Epoch time = 74.956s\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "CLIP = 1 # gradient clipping\n",
    "RESUME = False\n",
    "\n",
    "train_iter = Hdf5Dataset(\n",
    "    pl.Path(folder) / train_filename, num_entries=N_TRAIN_SAMPLES)\n",
    "train_dataloader = DataLoader(\n",
    "    train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "val_iter = Hdf5Dataset(pl.Path(folder) / valid_filename, num_entries=N_VAL_SAMPLES)\n",
    "val_dataloader = DataLoader(\n",
    "    val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "# make sure folder exists\n",
    "pl.Path(\"checkpoints\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if RESUME:\n",
    "    checkpoint = torch.load(\n",
    "        pl.Path(\"checkpoints\") /\n",
    "        f\"model-epoch_{NUM_EPOCHS-1}-{current_time()}.pt\"\n",
    "    )\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    start_time = timer()\n",
    "    print(\n",
    "        f\"\\033[92mEpoch {epoch} of {NUM_EPOCHS} - time: {current_time()}\\033[0m\")\n",
    "    print(f\"\\033[92mTraining...\\033[0m\")\n",
    "    train_loss = train(model, train_dataloader, optimizer, loss_fn, CLIP)\n",
    "    end_time = timer()\n",
    "    print(f\"\\033[92mValidating...\\033[0m\")\n",
    "    val_loss = evaluate(model, val_dataloader, loss_fn)\n",
    "    print(\n",
    "        (\n",
    "            f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"\n",
    "            f\"Epoch time = {(end_time - start_time):.3f}s\"\n",
    "        )\n",
    "    )\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"loss\": val_loss,\n",
    "        },\n",
    "        pl.Path(\"checkpoints\") /\n",
    "        f\"model-epoch_{NUM_EPOCHS-1}-{current_time()}.pt\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "0TE0FdC5VOBx"
   },
   "outputs": [],
   "source": [
    "# function to generate output sequence using greedy algorithm\n",
    "def correct_sentence_vectorized(src_tensor, model, max_len=50):\n",
    "    assert isinstance(src_tensor, torch.Tensor)\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model = model.module # get the accessor if the model was run in wrapped dataparallel obj\n",
    "    model.eval()\n",
    "    src_tensor = src_tensor.unsqueeze(1).to(DEVICE)\n",
    "    # get length of input sentence\n",
    "    src_len = src_tensor.shape[0]\n",
    "    trg_vocab_size = model.decoder.output_dim\n",
    "    # tensor to store decoder outputs\n",
    "    outputs = torch.zeros(max_len, 1, trg_vocab_size).to(DEVICE)\n",
    "    # last hidden state of the encoder is the context\n",
    "    with torch.no_grad():\n",
    "        _, enc_hidden = model.encoder(src_tensor)\n",
    "    # context also used as the initial hidden state of the decoder\n",
    "    context = enc_hidden\n",
    "    dec_hidden = enc_hidden\n",
    "    # first input to the decoder is the <sos> tokens\n",
    "    dec_input = src_tensor[0]\n",
    "    # enc_src = [batch_sz, src_len, hid_dim]\n",
    "    # Even though some examples might have been completed by producing a <eos> token\n",
    "    # we still need to feed them through the model because other are not yet finished\n",
    "    # and all examples act as a batch. Once every single sentence prediction encounters\n",
    "    # <eos> token, then we can stop predicting.\n",
    "    for t in range(1, max_len+1):\n",
    "        # insert input token embedding, previous hidden state and the context state\n",
    "        # receive output tensor (predictions) and new hidden state\n",
    "        dec_output, dec_hidden = model.decoder(dec_input, context, dec_hidden)\n",
    "        # place predictions in a tensor holding predictions for each token\n",
    "        outputs[t-1] = dec_output\n",
    "        # get the highest predicted token from our predictions\n",
    "        top_token_id = dec_output.argmax(1)\n",
    "        dec_input = top_token_id\n",
    "\n",
    "    pred_sentence = []\n",
    "    for i in range(0, len(outputs)):\n",
    "        topv, topi = outputs[i, :, :].topk(1)\n",
    "        pred_sentence.append(vocab_transform.vocab.itos_[topi])\n",
    "        if topi == EOS_IDX:\n",
    "            break\n",
    "    return \" \".join(pred_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3dDyDJ4UVeL4",
    "outputId": "dd58acf2-4979-4799-8072-fe06c6981d4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/model-epoch_9-2023-08-11-13:33:09.pt\n",
      "input: \" I think I'm goign to have to inform your posts a few times in order to gain even a small appreciation on how it all fits togethed but it looks very interesting! \"\n",
      "target: \" I think I'm going to have to re-read your posts a few times in order to gain even a small understanding of how it all fits together but it certainly looks very interesting! \"\n",
      "\u001b[91mModel's prediction: \u001b[0mfever i am my my <UNK> <UNK> <UNK> in in my my i i i it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it\n"
     ]
    }
   ],
   "source": [
    "latest_checkpoint = sorted(Path(\"checkpoints\").glob(\"*.pt\"), key=os.path.getmtime)[-1]\n",
    "print(latest_checkpoint)\n",
    "checkpoint = torch.load(latest_checkpoint)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Pick one in 18M examples\n",
    "val_iter = Hdf5Dataset(pl.Path(folder) / valid_filename, num_entries=5)\n",
    "val_iter = iter(val_iter)\n",
    "src, trg = next(val_iter)\n",
    "#src, trg = np.random.choice(val_iter)\n",
    "\n",
    "print('input: \"', src, '\"')\n",
    "print('target: \"', trg, '\"')\n",
    "\n",
    "src = text_transform(src)\n",
    "\n",
    "print(f\"\\033[91mModel's prediction: \\033[0m\", end=\"\")\n",
    "print(correct_sentence_vectorized(src, model))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
